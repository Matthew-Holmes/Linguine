{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling vocabulary\n",
    "\n",
    "In this notebook we develop a model of a users likelihood of knowing a word, given it's occurence frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model requirements\n",
    "\n",
    "We have verified that the Zipf-Mandelbrot model is valid for our data, so now we need to create a model of the following\n",
    "\n",
    "$$\n",
    "\\mathbb{P}_{\\text{recognise}}(\\text{word}) = f(\\text{Frequency}(\\text{word}))\n",
    "$$\n",
    "\n",
    "Where $f$ is the function to model. We expect that:\n",
    "\n",
    "$$\n",
    "\\sum_{\\text{words}} \\mathbb{P}_{\\text{recognise}}(\\text{word}) = \\text{vocabulary size}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper: https://journals.sagepub.com/doi/10.1177/0963721417727521#bibr1-0963721417727521\n",
    "\n",
    "models $f$ as a logistic function of the zipf-score, which is the log10 frequency per million words, +3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "A logistic curve has the formula (x being zipf-score):\n",
    "\n",
    "$$\n",
    "L(x) = \\frac{L}{1 + e^{-k(x-x_0)}}\n",
    "$$\n",
    "\n",
    "From the paper the variables look to roughly be for a low vocabulary learner:\n",
    "* $L =1$\n",
    "* $x_0 = 4$\n",
    "* $k = 2$\n",
    "\n",
    "For a medium vocabulary learner:\n",
    "* $L=1$\n",
    "* $x_0=2$\n",
    "* $k = 1.8$\n",
    "\n",
    "And a high vocabulary learner:\n",
    "* $L=1$\n",
    "* $x_0 = 1.5$\n",
    "* $k = 1.3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider $x_0$ to be the $\\mathbb{P}_{50}$, the Zipf value at which a word is 50% likely to be understood, then $k$ is a measure of how \"taily\" the distribution is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using this \n",
    "\n",
    "The goal of this exercise is to:\n",
    "* create a method to determine a user's initial vocabulary\n",
    "* create a model to assign probababilities that a user knows a word, without testing them\n",
    "\n",
    "Then we can track exposures/testing of the word, to update these probabilities, and drive which words to introduce to the user. Then by considering: word frequency in target media; probability improvement under testing; and existing probability known - produce a policy for teaching that adapts to the user's needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inital vocabulary estimation\n",
    "\n",
    "The most basic approach will be to test on a subset of words at each Zipf score level, to model the probability a user knows the word.\n",
    "\n",
    "Then fit a Zipf-Mandelbrot model to the definitions we have, then sum all the word's probabilities, to get the vocabulary size. \n",
    "\n",
    "The problem is that we only have limited text so far, so this might be tricky as we have to predict frequencies for all the unseen words. Observe that we actually don't need a vocabulary counter, only the probabilities based on frequency - which we always have for words that have been processed!\n",
    "\n",
    "**Conclusion** - we settle for a model that predicts the probability a user knows a word based on its occurence frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then consider the following score for a word:\n",
    "\n",
    "$$\n",
    "\\text{LearnerScore(\\text{Word})} = \\text{Frequency}(\\text{Word}) \\cdot  \\mathbb{P}\\delta_{\\text{Learner}}(\\text{Word})\n",
    "$$\n",
    "\n",
    "Where $\\mathbb{P}\\delta_{Learner}$ is measure of the how much the probability of knowing the word increases, per unit \"learner effort\" - which will be left suitable nebulous as a definition.\n",
    "\n",
    "Words with the highest learner score can then be targeted, with the frequencies tuned for the texts/topics of interest. This will optimise the expected number of words understood in a text, as a function of effort spent on learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
